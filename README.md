# Thai Speech Emotion Recognition (SER) — CNN–BiLSTM–Attention

โปรเจกต์นี้คือระบบจำแนกอารมณ์จากเสียงพูดภาษาไทย (Speech Emotion Recognition) โดยใช้สเปกโตรแกรมแบบ Log-Mel เป็นอินพุต และสถาปัตยกรรมแบบ CNN–BiLSTM–Attention เพื่อจับทั้งโครงสร้างเชิงสเปกตรัมและบริบทตามเวลา แนวคิดและเนื้อหาถูกสรุปจากรายงานฉบับเต็มที่แนบมา

## ภาพรวมและแรงบันดาลใจ

- เป้าหมาย: ทำให้ระบบสามารถ “ฟังเป็น” เข้าใจอารมณ์ของผู้พูด (เช่น เด็ก) เพื่อโต้ตอบให้เหมาะสมกับสถานการณ์จริง เช่น ปลอบเมื่อเศร้า ชวนคุยเบาๆ เมื่อหงุดหงิด หรือเสริมแรงเชิงบวกเมื่อมีความสุข
- ความท้าทายภาษาไทย: เป็นภาษาวรรณยุกต์ ทำให้การแยกระหว่างการเปลี่ยนแปลงของโทนเสียงจาก “อารมณ์” กับ “วรรณยุกต์ของคำ” ทำได้ยากกว่าภาษาอังกฤษ อีกทั้งข้อมูลที่ติดฉลากอารมณ์คุณภาพสูงมีจำกัดและมีปัญหา class imbalance ได้ง่าย
- วิธีที่เลือก: ใช้ Deep Learning กับสเปกโตรแกรม Log-Mel ให้โมเดลเรียนรู้ฟีเจอร์เอง จับบริบทยาวตามเวลาด้วย BiLSTM และเพิ่มความสามารถในการตีความ/โฟกัสด้วย Attention พร้อมแนวทางประเมินผลที่ทำซ้ำได้ (reproducible)

## คุณสมบัติเด่นของวิธี

- เรียนรู้ตัวแทน (representation) จากสเปกโตรแกรมโดยตรงด้วย CNN แทนการพึ่งพา hand-crafted features เพียงอย่างเดียว
- จับบริบทยาวตามเวลาได้ดีผ่าน BiLSTM และกลไก Attention ช่วยเน้นส่วนสำคัญของสัญญาณ
- ใช้เมตริกที่เหมาะกับ class imbalance เช่น Macro-F1 และ UAR (Unweighted Average Recall)

## สถาปัตยกรรมโมเดล (ย่อ)

- อินพุต: Log-Mel Spectrogram ขนาดประมาณ 1 × 64 × T (โมโน, 64 mel bins, T คือจำนวนเฟรมเวลา)
- ส่วนสกัดฟีเจอร์เชิงสเปกตรัม: CNN หลายชั้น + MaxPool ลดมิติลงทีละขั้น จนได้เทนเซอร์ที่สรุปโครงสร้างสเปกตรัม
- การฉายสู่ลำดับเวลา: ปรับรูปร่างเป็นลำดับของเวกเตอร์ต่อ time step
- ส่วนจับบริบทตามเวลา: BiLSTM ตามด้วย Attention เพื่อโฟกัสช่วงเวลาที่สำคัญต่อการตัดสินอารมณ์
- ส่วนจำแนก: Fully Connected + Softmax สำหรับ K คลาส

> หมายเหตุ: รายละเอียดชั้นและจำนวนพารามิเตอร์เฉพาะ ดูได้จากสมุดโน้ต `project_deep_learning.ipynb` และรายงานฉบับเต็ม

## ป้ายกำกับอารมณ์ (label map)

โปรเจกต์นี้ใช้ป้ายกำกับดังต่อไปนี้ (อ้างอิงจาก `test_metrics.json`):

- Angry (0)
- Frustrated (1)
- Happy (2)
- Neutral (3)
- Sad (4)
- other (5)

## ผลลัพธ์สรุป (จากชุดทดสอบ)

ค่าเมตริกต่อไปนี้มาจากไฟล์ `test_metrics.json`:

- Test loss: 1.0199
- Macro-F1: 0.6007
- UAR: 0.5051

นอกจากนี้ยังควรพิจารณา Confusion Matrix และ Attention map (ถ้ามีบันทึกไว้ในสมุดโน้ต) เพื่อดูพฤติกรรมโมเดลเชิงลึก

## ข้อมูลชุดข้อมูลและแหล่งที่มา

- ใช้คอร์ปัส THAI-SER (ประมาณ 41.6 ชม., 27,854 utterances, นักแสดง 200 คน; สภาพแวดล้อม Studio A/B และ Zoom)
- มีเมตาดาต้าอธิบายชนิดไมค์/เซสชัน และการติดป้ายกำกับจากคะแนนโหวตหลายคน (majority vote)
- ลิงก์ชุดข้อมูล (ประกาศทางการ / releases):
	- vistec-AI/dataset-releases – THAI SER (Releases): https://github.com/vistec-AI/dataset-releases/releases

## สิทธิ์การใช้งานชุดข้อมูล (Dataset License)

การใช้งานชุดข้อมูล THAI-SER อยู่ภายใต้เงื่อนไข/สัญญาอนุญาตที่กำหนดโดยผู้เผยแพร่ (vistec-AI) โปรดอ่านรายละเอียดในหน้า Releases/ไฟล์ LICENSE ที่เกี่ยวข้องก่อนดาวน์โหลดหรือใช้งาน เพื่อทำความเข้าใจข้อกำหนดด้านการอ้างอิง การใช้งานเชิงพาณิชย์ การแจกจ่ายซ้ำ และข้อจำกัดอื่นๆ ที่อาจระบุไว้

## วิธีใช้งานอย่างย่อ

1) เตรียมสภาพแวดล้อม Python (แนะนำ 3.10+) และติดตั้งไลบรารีพื้นฐานที่เกี่ยวกับสัญญาณเสียงและ PyTorch
2) เปิดและรัน `project_deep_learning.ipynb` ตามลำดับเซลล์ เพื่อดูขั้นตอนตั้งแต่การโหลดข้อมูล การสร้างสเปกโตรแกรม Log-Mel การฝึก และการประเมินผล
3) หากต้องการโหลดน้ำหนักที่ดีที่สุด เพื่อนำไปประเมินซ้ำหรือทดสอบกับไฟล์เสียงใหม่ ให้ปรับโค้ดในส่วนโหลดโมเดลให้ชี้ไปที่ `best_cnn_bilstm_attn.pt`

> เคล็ดลับ: หากข้อมูลมี class imbalance ให้ใช้ class weights, data augmentation และเมตริกอย่าง Macro-F1/UAR เพื่อติดตามประสิทธิภาพอย่างยุติธรรม

## หมายเหตุและข้อจำกัด

- ภาษาไทยมีวรรณยุกต์ โมเดลจึงต้องระวังการแยกแยะ “อารมณ์” ออกจาก “วรรณยุกต์ของคำ” โดยเฉพาะในสถานการณ์จริงที่มีสภาพแวดล้อมและไมโครโฟนหลากหลาย
- หากย้ายโดเมนหรือสำเนียง (domain shift) ควรปรับจูน (fine-tune) หรือทำ data augmentation เพื่อคงความทนทาน (robustness)

## อ้างอิง

- เนื้อหาและรายละเอียดเชิงลึกอ้างอิงจากรายงานฉบับเต็มที่แนบ (`final_report.pdf`) และโน้ตบุ๊ก `project_deep_learning.ipynb`

—
